{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Imageclassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishnuBhaarath/Computervisionprojects/blob/master/Imageclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxwLO7kWOcdF",
        "colab_type": "code",
        "outputId": "a54df055-de39-4d26-ac31-4e1138468ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Extracting images from the zip file\n",
        "from zipfile import ZipFile\n",
        "filename= \"marvel.zip\"\n",
        "with ZipFile(filename,'r')as zip:\n",
        "  zip.extractall()\n",
        "  print('extracted successfully')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "extracted successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-unJOoTLP7li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing \n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        " \n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "base_dir = os.path.join(os.path.dirname(filename), 'marvel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjcfDNd7QGeX",
        "colab_type": "code",
        "outputId": "f84d2874-fe95-4bab-b0ae-dac694eec76d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# Accessing the images and setting 0.7 of images for training and the rest for testing\n",
        "classes=['Black Widow','Captain America','Hulk','Iron Man','Thor']\n",
        "for m in classes:\n",
        "  img_path = os.path.join(base_dir, m)\n",
        "  images = glob.glob(img_path + '/*.jpg')\n",
        "  print(\"{}: {} Images\".format(m, len(images)))\n",
        "  num_train = int(round(len(images)*0.7))\n",
        "  train, val = images[:num_train], images[num_train:]\n",
        "\n",
        "# Creating seperate directories for training data\n",
        "  for t in train:\n",
        "    if not os.path.exists(os.path.join(base_dir, 'train', m)):\n",
        "      os.makedirs(os.path.join(base_dir, 'train', m))\n",
        "    shutil.move(t, os.path.join(base_dir, 'train', m))\n",
        "# Creating seperate directories for validating data\n",
        "  for v in val:\n",
        "    if not os.path.exists(os.path.join(base_dir, 'val', m)):\n",
        "      os.makedirs(os.path.join(base_dir, 'val', m))\n",
        "    shutil.move(v, os.path.join(base_dir, 'val', m))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Black Widow: 0 Images\n",
            "Captain America: 0 Images\n",
            "Hulk: 0 Images\n",
            "Iron Man: 0 Images\n",
            "Thor: 0 Images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFLmBKceQ4rm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting train directory and validate directory\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ce9DzUVRCrP",
        "colab_type": "code",
        "outputId": "d74b9f5c-7493-44d5-cb0e-66f5dfdcc2b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Setting batch size and a constant image shape\n",
        "batch_size = 130\n",
        "IMG_SHAPE = 150 \n",
        "# Rescaling the images so all the values lie between 0 and 1 and applying horizontal flip and training the data\n",
        "image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
        "\n",
        "train_data_gen = image_gen.flow_from_directory(\n",
        "                                                batch_size=batch_size,\n",
        "                                                directory=train_dir,\n",
        "                                                shuffle=True,\n",
        "                                                target_size=(IMG_SHAPE,IMG_SHAPE)\n",
        "                                                )\n",
        "# Rescaling the images so all the values lie between 0 and 1 and rotating and training the data\n",
        "image_gen = ImageDataGenerator(rescale=1./255, rotation_range=45)\n",
        "\n",
        "train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n",
        "                                               directory=train_dir,\n",
        "                                               shuffle=True,\n",
        "                                               target_size=(IMG_SHAPE, IMG_SHAPE))\n",
        "#Rescaling and zooming the data\n",
        "image_gen_train = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=45,\n",
        "                    width_shift_range=.15,\n",
        "                    height_shift_range=.15,\n",
        "                    horizontal_flip=True,\n",
        "                    zoom_range=0.5\n",
        "                    )\n",
        "\n",
        "\n",
        "train_data_gen = image_gen_train.flow_from_directory(\n",
        "                                                batch_size=batch_size,\n",
        "                                                directory=train_dir,\n",
        "                                                shuffle=True,\n",
        "                                                target_size=(IMG_SHAPE,IMG_SHAPE),\n",
        "                                                class_mode='sparse'\n",
        "                                                )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 105 images belonging to 5 classes.\n",
            "Found 105 images belonging to 5 classes.\n",
            "Found 105 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIMylCxXR4Rj",
        "colab_type": "code",
        "outputId": "a5bc0370-ba43-49d2-9ade-8ff4c9cc7fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n",
        "                                                 directory=val_dir,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='sparse')\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_SHAPE,IMG_SHAPE, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, 3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# Adding dropout to turn down some neurons\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3008e39f4a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_gen_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                                  \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                  \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SHAPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9alQ3HvGS1MJ",
        "colab_type": "code",
        "outputId": "bec9d7ee-9981-4099-c9f2-97f9830e0cb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "epochs = 120\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=int(np.ceil(train_data_gen.n / float(batch_size))),\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(val_data_gen.n / float(batch_size)))\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3300 - accuracy: 0.8857 - val_loss: 2.6098 - val_accuracy: 0.3778\n",
            "Epoch 2/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 2.1213 - accuracy: 0.5238 - val_loss: 1.5781 - val_accuracy: 0.6000\n",
            "Epoch 3/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5977 - accuracy: 0.7524 - val_loss: 3.2129 - val_accuracy: 0.3556\n",
            "Epoch 4/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7823 - accuracy: 0.5238 - val_loss: 2.3242 - val_accuracy: 0.4222\n",
            "Epoch 5/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2769 - accuracy: 0.5810 - val_loss: 1.4143 - val_accuracy: 0.6000\n",
            "Epoch 6/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5131 - accuracy: 0.8095 - val_loss: 1.3073 - val_accuracy: 0.5333\n",
            "Epoch 7/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5687 - accuracy: 0.7143 - val_loss: 1.3587 - val_accuracy: 0.5333\n",
            "Epoch 8/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.8560 - accuracy: 0.6857 - val_loss: 1.3562 - val_accuracy: 0.5111\n",
            "Epoch 9/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8902 - accuracy: 0.6286 - val_loss: 1.2305 - val_accuracy: 0.5556\n",
            "Epoch 10/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7194 - accuracy: 0.7048 - val_loss: 1.0791 - val_accuracy: 0.5778\n",
            "Epoch 11/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6473 - accuracy: 0.7619 - val_loss: 1.0227 - val_accuracy: 0.5556\n",
            "Epoch 12/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5518 - accuracy: 0.8190 - val_loss: 1.0594 - val_accuracy: 0.5778\n",
            "Epoch 13/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6070 - accuracy: 0.7810 - val_loss: 1.1307 - val_accuracy: 0.6000\n",
            "Epoch 14/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6009 - accuracy: 0.8000 - val_loss: 1.1670 - val_accuracy: 0.6000\n",
            "Epoch 15/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6703 - accuracy: 0.8095 - val_loss: 1.1518 - val_accuracy: 0.5778\n",
            "Epoch 16/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6237 - accuracy: 0.7714 - val_loss: 1.1101 - val_accuracy: 0.5556\n",
            "Epoch 17/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5671 - accuracy: 0.8381 - val_loss: 1.0467 - val_accuracy: 0.6222\n",
            "Epoch 18/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5650 - accuracy: 0.8095 - val_loss: 0.9963 - val_accuracy: 0.6222\n",
            "Epoch 19/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4978 - accuracy: 0.8476 - val_loss: 0.9629 - val_accuracy: 0.6000\n",
            "Epoch 20/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5336 - accuracy: 0.8286 - val_loss: 0.9452 - val_accuracy: 0.6000\n",
            "Epoch 21/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5264 - accuracy: 0.8095 - val_loss: 0.9276 - val_accuracy: 0.6222\n",
            "Epoch 22/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4836 - accuracy: 0.8571 - val_loss: 0.9128 - val_accuracy: 0.6222\n",
            "Epoch 23/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4272 - accuracy: 0.8952 - val_loss: 0.9036 - val_accuracy: 0.6444\n",
            "Epoch 24/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4583 - accuracy: 0.8190 - val_loss: 0.9055 - val_accuracy: 0.6222\n",
            "Epoch 25/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4893 - accuracy: 0.8190 - val_loss: 0.9163 - val_accuracy: 0.6444\n",
            "Epoch 26/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4917 - accuracy: 0.8571 - val_loss: 0.9425 - val_accuracy: 0.6222\n",
            "Epoch 27/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4809 - accuracy: 0.8000 - val_loss: 0.9789 - val_accuracy: 0.6444\n",
            "Epoch 28/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3686 - accuracy: 0.9048 - val_loss: 1.0102 - val_accuracy: 0.6222\n",
            "Epoch 29/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4713 - accuracy: 0.8286 - val_loss: 1.0186 - val_accuracy: 0.6222\n",
            "Epoch 30/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4354 - accuracy: 0.8476 - val_loss: 1.0175 - val_accuracy: 0.6444\n",
            "Epoch 31/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4503 - accuracy: 0.8381 - val_loss: 1.0168 - val_accuracy: 0.6222\n",
            "Epoch 32/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3627 - accuracy: 0.8857 - val_loss: 1.0082 - val_accuracy: 0.6222\n",
            "Epoch 33/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3877 - accuracy: 0.8857 - val_loss: 1.0132 - val_accuracy: 0.6222\n",
            "Epoch 34/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3633 - accuracy: 0.8667 - val_loss: 1.0271 - val_accuracy: 0.6000\n",
            "Epoch 35/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4152 - accuracy: 0.8571 - val_loss: 1.0688 - val_accuracy: 0.6000\n",
            "Epoch 36/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3486 - accuracy: 0.8857 - val_loss: 1.1001 - val_accuracy: 0.6000\n",
            "Epoch 37/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4185 - accuracy: 0.8571 - val_loss: 1.1025 - val_accuracy: 0.5778\n",
            "Epoch 38/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3147 - accuracy: 0.9048 - val_loss: 1.0390 - val_accuracy: 0.6000\n",
            "Epoch 39/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3230 - accuracy: 0.8952 - val_loss: 0.9971 - val_accuracy: 0.6444\n",
            "Epoch 40/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3226 - accuracy: 0.8952 - val_loss: 0.9829 - val_accuracy: 0.6444\n",
            "Epoch 41/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2842 - accuracy: 0.9048 - val_loss: 0.9803 - val_accuracy: 0.6889\n",
            "Epoch 42/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3525 - accuracy: 0.8571 - val_loss: 0.9747 - val_accuracy: 0.6667\n",
            "Epoch 43/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3250 - accuracy: 0.9048 - val_loss: 0.9826 - val_accuracy: 0.6222\n",
            "Epoch 44/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3582 - accuracy: 0.8762 - val_loss: 1.0426 - val_accuracy: 0.6000\n",
            "Epoch 45/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3169 - accuracy: 0.9048 - val_loss: 1.1032 - val_accuracy: 0.6000\n",
            "Epoch 46/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2922 - accuracy: 0.8857 - val_loss: 1.1139 - val_accuracy: 0.6222\n",
            "Epoch 47/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3189 - accuracy: 0.9238 - val_loss: 1.0870 - val_accuracy: 0.6222\n",
            "Epoch 48/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2986 - accuracy: 0.8857 - val_loss: 1.1120 - val_accuracy: 0.5778\n",
            "Epoch 49/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2237 - accuracy: 0.9333 - val_loss: 1.1610 - val_accuracy: 0.5778\n",
            "Epoch 50/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2826 - accuracy: 0.9048 - val_loss: 1.1282 - val_accuracy: 0.6222\n",
            "Epoch 51/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2701 - accuracy: 0.9143 - val_loss: 1.1227 - val_accuracy: 0.6000\n",
            "Epoch 52/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3398 - accuracy: 0.8667 - val_loss: 1.0934 - val_accuracy: 0.6222\n",
            "Epoch 53/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2776 - accuracy: 0.8952 - val_loss: 1.1019 - val_accuracy: 0.6222\n",
            "Epoch 54/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3037 - accuracy: 0.9048 - val_loss: 1.1174 - val_accuracy: 0.6222\n",
            "Epoch 55/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3553 - accuracy: 0.9048 - val_loss: 1.1255 - val_accuracy: 0.6000\n",
            "Epoch 56/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2492 - accuracy: 0.9048 - val_loss: 1.1309 - val_accuracy: 0.6222\n",
            "Epoch 57/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3314 - accuracy: 0.8762 - val_loss: 1.0713 - val_accuracy: 0.6667\n",
            "Epoch 58/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3533 - accuracy: 0.8381 - val_loss: 1.0519 - val_accuracy: 0.6667\n",
            "Epoch 59/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2385 - accuracy: 0.9143 - val_loss: 1.0327 - val_accuracy: 0.7111\n",
            "Epoch 60/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3345 - accuracy: 0.8476 - val_loss: 1.0541 - val_accuracy: 0.6889\n",
            "Epoch 61/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2193 - accuracy: 0.9333 - val_loss: 1.1380 - val_accuracy: 0.6000\n",
            "Epoch 62/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2639 - accuracy: 0.9143 - val_loss: 1.1564 - val_accuracy: 0.5778\n",
            "Epoch 63/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2747 - accuracy: 0.9238 - val_loss: 1.0575 - val_accuracy: 0.6667\n",
            "Epoch 64/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2357 - accuracy: 0.9143 - val_loss: 1.0073 - val_accuracy: 0.6667\n",
            "Epoch 65/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2720 - accuracy: 0.9429 - val_loss: 1.0022 - val_accuracy: 0.6667\n",
            "Epoch 66/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2922 - accuracy: 0.9048 - val_loss: 1.0558 - val_accuracy: 0.6000\n",
            "Epoch 67/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3601 - accuracy: 0.8571 - val_loss: 1.0465 - val_accuracy: 0.6000\n",
            "Epoch 68/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1638 - accuracy: 0.9524 - val_loss: 0.9949 - val_accuracy: 0.6444\n",
            "Epoch 69/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2525 - accuracy: 0.8952 - val_loss: 0.9333 - val_accuracy: 0.6667\n",
            "Epoch 70/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1930 - accuracy: 0.9524 - val_loss: 0.9549 - val_accuracy: 0.6889\n",
            "Epoch 71/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2778 - accuracy: 0.8762 - val_loss: 0.9818 - val_accuracy: 0.6889\n",
            "Epoch 72/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2117 - accuracy: 0.9238 - val_loss: 1.0237 - val_accuracy: 0.6444\n",
            "Epoch 73/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2568 - accuracy: 0.9333 - val_loss: 1.1298 - val_accuracy: 0.6222\n",
            "Epoch 74/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2741 - accuracy: 0.9143 - val_loss: 1.2005 - val_accuracy: 0.6222\n",
            "Epoch 75/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2539 - accuracy: 0.8952 - val_loss: 1.0792 - val_accuracy: 0.6667\n",
            "Epoch 76/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1999 - accuracy: 0.9238 - val_loss: 1.0198 - val_accuracy: 0.7111\n",
            "Epoch 77/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2706 - accuracy: 0.9143 - val_loss: 1.0164 - val_accuracy: 0.7111\n",
            "Epoch 78/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3037 - accuracy: 0.8857 - val_loss: 1.0122 - val_accuracy: 0.7111\n",
            "Epoch 79/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2208 - accuracy: 0.9429 - val_loss: 1.0596 - val_accuracy: 0.6667\n",
            "Epoch 80/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2276 - accuracy: 0.8952 - val_loss: 1.0421 - val_accuracy: 0.6667\n",
            "Epoch 81/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2754 - accuracy: 0.8952 - val_loss: 0.9536 - val_accuracy: 0.7111\n",
            "Epoch 82/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3069 - accuracy: 0.9143 - val_loss: 0.9901 - val_accuracy: 0.7333\n",
            "Epoch 83/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2292 - accuracy: 0.9429 - val_loss: 1.0186 - val_accuracy: 0.7333\n",
            "Epoch 84/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2443 - accuracy: 0.8952 - val_loss: 1.0159 - val_accuracy: 0.7111\n",
            "Epoch 85/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2669 - accuracy: 0.9238 - val_loss: 1.0734 - val_accuracy: 0.6889\n",
            "Epoch 86/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1781 - accuracy: 0.9524 - val_loss: 1.1209 - val_accuracy: 0.6667\n",
            "Epoch 87/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2133 - accuracy: 0.9143 - val_loss: 1.1011 - val_accuracy: 0.6889\n",
            "Epoch 88/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2127 - accuracy: 0.9143 - val_loss: 1.1015 - val_accuracy: 0.7556\n",
            "Epoch 89/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1566 - accuracy: 0.9714 - val_loss: 1.1161 - val_accuracy: 0.7333\n",
            "Epoch 90/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2442 - accuracy: 0.9333 - val_loss: 1.0910 - val_accuracy: 0.7333\n",
            "Epoch 91/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2421 - accuracy: 0.9238 - val_loss: 1.0990 - val_accuracy: 0.6889\n",
            "Epoch 92/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1525 - accuracy: 0.9429 - val_loss: 1.1604 - val_accuracy: 0.6444\n",
            "Epoch 93/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2152 - accuracy: 0.9238 - val_loss: 1.1815 - val_accuracy: 0.6222\n",
            "Epoch 94/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1825 - accuracy: 0.9524 - val_loss: 1.2209 - val_accuracy: 0.6222\n",
            "Epoch 95/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1911 - accuracy: 0.9238 - val_loss: 1.1549 - val_accuracy: 0.6444\n",
            "Epoch 96/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2903 - accuracy: 0.9143 - val_loss: 1.0873 - val_accuracy: 0.6444\n",
            "Epoch 97/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1686 - accuracy: 0.9333 - val_loss: 1.0822 - val_accuracy: 0.6444\n",
            "Epoch 98/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2122 - accuracy: 0.9333 - val_loss: 1.0690 - val_accuracy: 0.6444\n",
            "Epoch 99/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1766 - accuracy: 0.9524 - val_loss: 1.0870 - val_accuracy: 0.6222\n",
            "Epoch 100/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1577 - accuracy: 0.9524 - val_loss: 1.1199 - val_accuracy: 0.6444\n",
            "Epoch 101/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1536 - accuracy: 0.9524 - val_loss: 1.1527 - val_accuracy: 0.6667\n",
            "Epoch 102/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1429 - accuracy: 0.9810 - val_loss: 1.1763 - val_accuracy: 0.6444\n",
            "Epoch 103/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1993 - accuracy: 0.9429 - val_loss: 1.1226 - val_accuracy: 0.6444\n",
            "Epoch 104/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1686 - accuracy: 0.9429 - val_loss: 1.0977 - val_accuracy: 0.6667\n",
            "Epoch 105/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1984 - accuracy: 0.9524 - val_loss: 1.1054 - val_accuracy: 0.6667\n",
            "Epoch 106/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2053 - accuracy: 0.9333 - val_loss: 1.1368 - val_accuracy: 0.6667\n",
            "Epoch 107/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1956 - accuracy: 0.9238 - val_loss: 1.1962 - val_accuracy: 0.6667\n",
            "Epoch 108/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1544 - accuracy: 0.9619 - val_loss: 1.2123 - val_accuracy: 0.6667\n",
            "Epoch 109/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1888 - accuracy: 0.9238 - val_loss: 1.2042 - val_accuracy: 0.6444\n",
            "Epoch 110/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1754 - accuracy: 0.9238 - val_loss: 1.2320 - val_accuracy: 0.6444\n",
            "Epoch 111/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1569 - accuracy: 0.9429 - val_loss: 1.2088 - val_accuracy: 0.6222\n",
            "Epoch 112/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1798 - accuracy: 0.9048 - val_loss: 1.1873 - val_accuracy: 0.6444\n",
            "Epoch 113/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0971 - accuracy: 0.9810 - val_loss: 1.2188 - val_accuracy: 0.6222\n",
            "Epoch 114/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1720 - accuracy: 0.9429 - val_loss: 1.2596 - val_accuracy: 0.6000\n",
            "Epoch 115/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1906 - accuracy: 0.9238 - val_loss: 1.3283 - val_accuracy: 0.6222\n",
            "Epoch 116/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2035 - accuracy: 0.9238 - val_loss: 1.3967 - val_accuracy: 0.6222\n",
            "Epoch 117/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1608 - accuracy: 0.9429 - val_loss: 1.3523 - val_accuracy: 0.6667\n",
            "Epoch 118/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1952 - accuracy: 0.9238 - val_loss: 1.3025 - val_accuracy: 0.6889\n",
            "Epoch 119/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1728 - accuracy: 0.9429 - val_loss: 1.3136 - val_accuracy: 0.6444\n",
            "Epoch 120/120\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1992 - accuracy: 0.9238 - val_loss: 1.3229 - val_accuracy: 0.6444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4sQ-hSzUuHi",
        "colab_type": "text"
      },
      "source": [
        "# PROBLEM STATEMENT : Classifying the images of different maravel characters by using neutral network with maximum accuracy.\n",
        "# MY APPROACH AND LOGIC: I have downloaded 30 images for captain america,black widow,Thor,Hulk and iron man I am uploading the folder and extracting it. Used tensor flow and other essential libraries for this process, usually the images downloaded are of different sizes and different colours, so I convert them to standard size of 150* 150.I used flatten to convert the pixels to a 1D array. The next task is the colour as the colours of grey images are represented by a 1 D array it is simple, but in this case there are colour images I followed the logic that every colour image is a mixture  of red,blue and green, now each colour is of 2D but we have 3 colurs so we end up to with 3 2D arrays. As now there are three colours the input shape parameter has arguments 150,150,3 ,the first two represents the size while the last one 3 represents the number of colours. I did convolution on the given array with a kernel size of 3*3, Then I am making different directories init to train andvalidate the images in such a way that 70%  of the images goes to testing while the rest goes to validation. I have also done max pooling on the convulated image with a size 2*2 and a stride value of 2. I am creating my own directories for train and validate once it is done, I am doing some image augmentation to reduce the risk of overfitting by image rotation, zooming and horizontal flip,,then I am predicting the output.I have also used dropouts to turn down certain neurons to avoid the problem of overfitting and to predict the result more accurate.\n",
        "#Tools used: google code laboratory,Google images for downloading images. \n",
        "#conclusion : I was not able to complete it on time because I have mid sem till saturday and then there was a bit of problem installing the set ups that is the reason why I shifted to codelab,Now I have done the project, I am getting an accuracy of around 70%, the accuracy could still go  up if I have used more images.\n",
        "#Acknowledgements: I started learning it from scratch and I am happy that I was able to reach to such a level in a span of 2 days, I have really worked hard for the past 2 days from understanding the concepts, I am thankful for databyte for providing me with such a wonderful oppurtunity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okhQfQIDT8uP",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}